# Description: Configuration file for training on 4 GPUS (at least 16GB of memory per GPU)

dataset:
  ds_name: wmt14
  src_lang: en
  tgt_lang: de
  split: -1 # Automatically handled

model:
  num_heads: 8
  d_model: 512
  d_k: 64
  d_v: 64
  d_ff: 2048
  dropout: 0.1
  n_encoder_layers: 6
  n_decoder_layers: 6
  src_vocab_size: 32000
  tgt_vocab_size: 32000

training:
  # original paper -> 100k steps, 25k src and 25k tgt tokens each step, 8 GPUs
  # here, we do 200k steps with 12.5k src/tgt tokens per step. One gradient update each two steps -> 100k total updates
  # 12.5 k tokens per step -> 3125 per gpu
  # so, effectively, we are kind of replicating their setup but with 4 GPUs through gradient accum
  max_global_steps: 200000
  tokens_per_step_per_gpu: 3125
  grad_accum_steps: 2
  n_gpus: 4
  lr: 1
  use_scheduler: true
  b1: 0.9
  b2: 0.98
  eps: 0.0000000001 # 1e-9
  warmup_steps: 4000
  checkpoint_dir: "checkpoints/distrib_wmt14"
  checkpoint_filename: "latest"
  checkpoint_save_filename: "checkpoint.pth"
  save_freq: 1000 # each 1000 steps
  label_smoothing: 0.1

eval:
  batch_size: 256
  checkpoint_dir: "checkpoints/distrib_wmt14"
  checkpoint_filename: "latest"
